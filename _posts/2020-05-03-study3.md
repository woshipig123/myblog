---
title: 分词工具
date: 2020-05-03 08:00:00
categories:
- token
tags:
- nlp
---

  有很多分词工具，于是我们就拿几个常用的分词做一下对比。

<!--more-->

[jieba](https://github.com/fxsjy/jieba)
目前用的主流分词工具，新版本加入百度深度学习框架。
{% highlight python %}
import jieba
jieba.enable_paddle()# 启动paddle模式。 0.40版之后开始支持，早期版本不支持
str="我爱我的祖国，为中华民族伟大复兴的中国梦而奋斗"
seg=jieba.cut(str,cut_all=False)
print(" ".join(seg))
输出：我 爱 我 的 祖国 ， 为 中华民族 伟大 复兴 的 中国 梦 而 奋斗
{% endhighlight %}

[pkuseg](https://github.com/lancopku/pkuseg-python)
北大开源工具，使用还不错。
{% highlight python %}
import pkuseg
seg=pkuseg.pkuseg()
text=seg.cut("我爱我的祖国，为中华民族伟大复兴的中国梦而奋斗")
print(" ".join(text))
输出：我 爱 我 的 祖国 ， 为 中华 民族 伟大 复兴 的 中国 梦 而 奋斗
{% endhighlight %}

[Hanlp](https://github.com/hankcs/HanLP)
功能很强大的一个中文nlp工具。
{% highlight python %}
import hanlp
tokenizer = hanlp.load('PKU_NAME_MERGED_SIX_MONTHS_CONVSEG')
" ".join(tokenizer("我爱我的祖国，为中华民族伟大复兴的中国梦而奋斗"))
输出：我 爱 我 的 祖国 ， 为 中华民族 伟大 复兴 的 中国 梦 而 奋斗
{% endhighlight %}

[fasthan](https://github.com/fastnlp/fastHan)
fastHan是基于fastNLP与pytorch实现的中文自然语言处理工具，像spacy一样调用方便。
{% highlight python %}
from fastHan import FastHan
model=FastHan()#可选模型base or large
str="我爱我的祖国，为中华民族伟大复兴的中国梦而奋斗"
seg=model(str,target="CWS")
输出：['我', '爱', '我', '的', '祖国', '，', '为', '中华', '民族', '伟大', '复兴', '的', '中国', '梦', '而', '奋斗']
{% endhighlight %}

[Lac](https://github.com/baidu/lac)
百度自然语言处理部研发的一款联合的词法分析工具。
{% highlight python %}
from LAC import LAC
lac = LAC(mode='seg')
str="我爱我的祖国，为中华民族伟大复兴的中国梦而奋斗"
seg=lac.run(str)
输出：我 爱 我的祖国 ， 为 中华民族 伟大 复兴 的 中国 梦 而 奋斗
{% endhighlight %}

简单介绍这几个分词工具，各有优劣，如何选择，就看大家需求了。
