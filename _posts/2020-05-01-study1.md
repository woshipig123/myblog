---
title: 文本增强
date: 2020-05-01 20:00:00
categories:
- study
tag:
- nlp
---

EDA文本数据增强的方法[《EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks》](https://arxiv.org/abs/1901.11196)
EDA提出四种操作来进行数据增强，以防止过拟合，并提高模型泛化能力：

1. 同义词替换（SR: Synonyms Replace）：不考虑stopwords，在句子中随机抽取n个词，然后从同义词词典中随机抽取同义词，并进行替换。

2. 随机插入(RI: Randomly Insert)：不考虑stopwords，随机抽取一个词，然后在该词的同义词集合中随机选择一个，插入原句子中的随机位置。该过程可以重复n次。

3. 随机交换(RS: Randomly Swap)：句子中，随机选择两个词，位置交换。该过程可以重复n次。

4. 随机删除(RD: Randomly Delete)：句子中的每个词，以概率p随机删除。


在作者实验中，仅仅使用50%的数据集就达到了100%的效果。对于四种技术，数据集很小时，每一种技术都能够有2-3%的提升，当数据集很大时，每一种技术也能够有1%的提升。根据作者的经验来看，不要改变超过1/4的词汇的前提下，模型的鲁棒性都能得到很大的提升。
当训练数据很小时，模型更容易过拟合，这时建议多生成一些数据增强的样本。当训练数据很大时，大量增加数据增强样本可能没有帮助，因为模型本身可能已经能够泛化。


[中文版EDA](https://github.com/zhanlaoban/EDA_NLP_for_Chinese)

推荐一款贼好用的文本增强工具[nlpcda](https://pypi.org/project/nlpcda/)
[github地址](https://github.com/425776024/nlpcda)
使用方法很简单，很容易上手，再次就不再详细介绍使用方法了。
